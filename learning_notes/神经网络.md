一、神经元模型：

1、M-P神经元模型：

神经元接收到来自n个其他神经元传递过来的**输入信号**，这些输入信号通过**带权重的连接进行传递**，神经元接收到的**总输入值将与神经元的阀值**进行比较，然后通过"激活函数" (activation function) 处理以产生神经元的输出.。

2、常见激活函数：

<img src="assets/常见激活函数.png" style="width:250px height:250px" />

 3、**多层前馈神经网络**：神经元之间不存在同层连接， 也不存在跨层连接。

注：输入层神经元仅是接受输入 ，隐含层和输出层神经元都是拥有激活函数的功能神经元.

二、误差逆传播（BackPropagation BP）

$$w_i \leftarrow w_i+\Delta w_i$$   $$\Delta w_i = \eta (y-\widehat{y})x_i$$    

$$\eta \in (0,1)$$称为学习率，控制着算法每一轮迭代中的更新步长，若太大则容易震荡，太小则收敛速度会过慢。

，感知机的输出为$$\widehat{y}$$

三、卷积神经网络-主要层次 

常见的CNN结构有LeNet-5、AlexNet、ZFNet、**VGGNet、GoogleNet**、ResNet等等

1、数据输入层(Input Layer):对输入的数据需要进行预处理操作 

​      常见3种数据预处理方式：**去均值、归一化**、PCA降维/白化

2、卷积计算层：CONV Layer

​      卷积 ：一组**固定的权重**和**窗口内数据**做**矩阵内积**后**求和**的过程

​      卷积神经网络中，输入是一个多通道图像

- 局部感知： 在进行计算的时候，将图片划分为一个个的区域进行计算/考虑；
- 参数共享机制：假设每个神经元**连接数据窗的权重**是固定的
- 滑动窗口重叠：降低窗口与窗口之间的边缘不平滑的特性。
- 固定每个神经元的连接权重，可以将神经元看成一个模板；也就是**每个神经元只关注一个特性**，需要计算的权重个数会大大的减少

3、ReLU激励层：ReLU Incentive Layer：将卷积层的输出结果做一次**非线性映射**， 也就是做一次“激活” 

  激励层建议：

- CNN尽量不要使用sigmoid，如果要使用，建议只在**全连接层**使用
- 首先使用ReLU，因为迭代速度快，但是有可能效果不佳
- 如果使用ReLU失效的情况下，考虑使用Leaky ReLu或者Maxout，此时一般情况都可以解决啦
- tanh激活函数在某些情况下有比较好的效果，但是应用场景比较少

4、池化层：Pooling Layer

- 通过逐步**减小表征的空间尺寸**来减小参数量和网络中的计算；池化层在每个特征图上独立操作。

- 使用池化层可以**压缩数据和参数的量**，减小过拟合

- 在池化层进行压缩减少特征数量一般采用两种策略：Max Pooling(最大池化)、Average Pooling(平均池化)

  ​     Max pool 池化层反向传播，除最大值处继承上层梯度外，其他位置置零。

  ​     Average Pool 需要把残差平均分成2*2=4份，传递到前边小区域的4个单元即可。  

5、全连接层：FC Layer：只会在尾部出现

6、Batch Normalization Layer（可能有，慎用）

​      期望我们的结果是服从高斯分布的，所以对神经元的输出进行一下修正，一般放到卷积层/FC层后，池化层前



数据增强方法：

1、水平翻转

2、随机裁剪

3、fancy PCA 

4、样本不均衡【Label shuffle 】

5、平移变换；旋转/仿射变换；高斯噪声、模糊处理；对颜色的数据增强 

6、训练和测试要协调 

图像分类：

图像检测

病理切变癌细胞

CT图像分类

CT病噪语义标注

图像描述，图像问答，图像生成（高清复原）



