#### 1、如何用极大似然概率解释交叉熵loss ？

1. logistic regression模型二分类问题的极大似然估计

   0-1分布：$$P\{X=k\}=P^k(1-P)^{1-k}$$

   似然函数：$L(p) =\prod\limits_{i=1}^{N}P^k(1-P)^{1-k}$

   取对数：$$L L(p) =\sum\limits_{i=1}^{n} klog(P) +(1-k)log(1-p)$$

   求偏导，解方程组可求得极大似然估计值

2. 信息论相关知识

- 信息熵：

  $$H(p)=-\sum\limits_{i=1}^{n}p(x_i)*log(p(x_i))$$

- KL散度-相对熵：

  对于同一个随机变量x有两个单独的概率分布p(x)、q(x)，相对熵衡量这两个分布的差异

  $$D_{KL}(p||q)=\sum\limits_{i=1}^{n}p(x_i)*log(\frac{p(x_i)}{q(x_i)})$$

  P通常用来表示样本的真实分布，q用来表示模型所预测的分布

- 交叉熵：一般用来求目标与预测值之间的差距

  可由KL散度得到：

  $$D_{KL}(p||q)=\sum\limits_{i=1}^{n}p(x_i)*log(p(x_i))-\sum\limits_{i=1}^{n}p(x_i)*log(q(x_i)) $$

  $$D_{KL}(p||q)=-H(p)+[-\sum\limits_{i=1}^{n}p(x_i)*log(q(x_i))] $$

  后一部分即交叉熵：$$H(p,q)=-\sum\limits_{i=1}^{n}p(x_i)*log(q(x_i))$$

  所以在优化过程中只要关注交叉熵即可。

- 在分类问题中的使用:

  每一个节点的输出归一到[0,1]之间，单独对每个节点进行计算，每个节点只有两种可能值，即为二项分布

  |       | 苹果 | 西瓜 |
  | ----- | ---- | ---- |
  | Label | 1    | 1    |
  | pred  | 0.4  | 0.6  |

  $$loss= -p(x_i)*log(q(x_i))-（1-p(x_i)）*log(1-q(x_i)$$

  $$loss = loss_{苹果}+loss_{西瓜}$$

  综述可得：交叉熵loss本质上为极大似然估计

#### 2、硬币问题为何不使用MLP而直接假设正反面概率p

MLP多层感知机的本质作用是构造多个平面进行切分，将一团线性不可分的数据不断的切分到线性可分；

对于硬币的概率问题，根据发生的现象，构造关于p的解析解，使发生这种现象的可能性最大，是一个求极值问题，用梯度下降迭代的方法找到极值即可

#### 3、面对未知分类问题，存在A，B，C三个模型，如何确定使用哪一个模型？

- 根据输入数据是否有标签，确定是监督学习还是无监督学习
- 确认是二分类还是多分类问题
- 训练数据量的大小
- 训练数据是相互之间有联系，还是各自独立
- 训练数据是否需降维、是否平衡、缺失值、异常值等
- 模型否支持增量式训练
- 模型是否支持并行运算
- 模型是否具有可解释性
- 训练模型所消耗的时间
- 交叉验证、Precision, Recall, ROC, AUC 等方式评估模型的好坏

