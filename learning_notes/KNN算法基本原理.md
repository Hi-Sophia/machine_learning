### 三、KNN算法基本原理

#####  1、基本原理

- 从训练集合中获取K个离待预测样本距离最近的样本数据；
- 根据获取得到的K个样本数据来预测当前待预测样本的目标属性值。

每个样本都可以用它最接近的k个邻居来代表。

KNN在做**分类预测**时，采用**多数表决法**

KNN在做**回归预测**时，采用**平均值法**。

##### 2、KNN三要素

- K值的选择： 一般根据样本分布选择一个较小的值，然后通过**交叉验证**来选择一个比较合适的最终值
- 距离的度量 ：一般使用欧氏距离(欧几里得距离) 
- 决策规则： 在分类预测时，一般采用**多数表决法**或**加权多数表决法**；而在做回归预测时，一般采用**平均值法**或**加权平均值法**。

##### 3、KNN算法实现

**蛮力实现(brute)**：计算预测样本到所有训练集样本的距离，然后选择最小的k个距

离即可得到K个最邻近点。

缺点：特征数比较多、样本数比较多的时候，算法的执行效率比较低。

**KD树(kd_tree)**：首先对训练数据进行建模，构建KD树，然后再根据建好的模型来获取邻近样本数据。

##### 4、KD tree

**中位数**就是一组数据从小到大排列中间的那个数字



##### 5、KNN参数说明：

| KNN参数     | 说明                                                         |
| ----------- | ------------------------------------------------------------ |
| n_neighbors | 邻近数目，默认为5                                            |
| weights     | 样本权重，可选参数: uniform(等权重)、distance(权重和距离成反比，越近影响越强)；默认为uniform |
| leaf_size   | 在使用KD_Tree的时候，叶子数量，默认为30                      |
| metric      | 样本之间距离度量公式，默认为minkowski（闵可夫斯基）； ；当参数p为2的时候，其实就是欧几里得距离 |

```python
def init(self, n_neighbors=5,

                 weights='uniform', algorithm='auto', leaf_size=30,

                 p=2, metric='minkowski', metric_params=None, n_jobs=1,

                 **kwargs):
```

### 