集成学习：将若干个学习器(分类器&回归器)组合之后产生一个新学习器 。集成算法的成功在于保证弱分类器的多样性

弱分类器：指那些分类准确率只稍微好于随机猜测的分类器（error<0.5）。

集成学习思想：Bagging、Boosting、Stacking

1、Bagging方法【Bootstrap Aggregating】：

在原始数据集上通过**有放回的抽样**的方式，重新选择出**S个新数据集**来分别训练**S个分类器**的集成技术。即新的数据集中允许存在重复数据。

Bagging方法训练出来的模型在**预测新样本分类**的时候，会使用**多数投票**或者**求均值**的方式来统计最终的分类结果

2、随机森林(Random Forest)

- 从样本集中用**bootstrap采样**选出n个样本
- 从所有属性中随机选择k个属性，选择出**最佳分割属性**作为节点创建决策树
- 重复以上两步m次，即建立m颗决策树
- 这m个决策树形成随机森林，通过**投票表决**决定数据数据哪一类。

3、Boosting（提升学习）

可以用于回归和分类问题，它每一步产生**弱预测模型**，并**加权累加**到总模型中。

如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，那么就称为**梯度提升**(Gradient boosting)

- AdaBoost：

 利用前一轮的弱学习器的误差来更新样本权重值，一轮一轮迭代

- GBDT：

  要求弱学习器必须是CART模型，GBDT在模型训练时，要求模型预测的样本损失尽可能小。

用一个初始值来学习一棵决策树，**叶子处可以得到预测的值，以及预测之后的残差**，然后后面的决策树就要基

于前面决策树的残差来学习，**直到预测值和真实值的残差为零**

4、stacking

训练一个模型用于**组合其他模型**的技术。

即首先训练出多个不同的模型，然后再以之前训练的各个模型的输出作为输入来训练一个新的模型，从而得到一个最终的模型【一般使用单层的logistic回归作为组合模型】

```python
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier
from mlxtend.classifier import StackingClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import pandas as pd
from sklearn.datasets import load_iris
url = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = pd.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
seed=7
num_trees = 100
kfold = KFold(n_splits=10,random_state=seed,shuffle=True)
#1、BaggingClassifier
# tree = DecisionTreeClassifier()
# model = BaggingClassifier(base_estimator=tree,n_estimators=num_trees,random_state=seed)
#2、RandomForestClassifier
#model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
#3、AdaBoostClassifier
#model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)
#4、voting_stacking
# estimators = []
# model1 = LogisticRegression()
# model2 = DecisionTreeClassifier()
# model3 = SVC()
# estimators.extend([("logistic",model1),("tree",model2),("svm",model3)])
# model = VotingClassifier(estimators)
# results = cross_val_score(model,X,Y,cv=kfold)
# print(results.mean())
#5、stacking
clf1 = KNeighborsClassifier(n_neighbors=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],
                          meta_classifier=lr,
                          use_probas=True,
                          average_probas=False)
for clf, label in zip([clf1, clf2, clf3, sclf],
                      ['KNN',
                       'Random Forest',
                       'Naive Bayes',
                       'StackingClassifier']):
    scores = cross_val_score(clf, X, Y, cv=3, scoring='accuracy')
    print("Accuracy: %0.2f (+/- %0.2f) [%s]"
          % (scores.mean(), scores.std(), label))

```

