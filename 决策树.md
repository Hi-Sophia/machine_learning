##### 一、决策树简介：

1、决策树分为两大类：分类树和回归树，前者用于**分类标签值**，后者用于**预测连续值**，常用算法有ID3、C4.5、CART等

2、决策树的生成是一个**预测连续值**

3、决策树学习的关键是**如何选择最优划分属性**

##### 二、决策树节点纯度度量：

样本集合为D，第K类样本所占的比例为k

1、信息增益(information gain)：

信息熵(entropy) ：$$Ent(D)=-\sum\limits_{k=1}^{N}p_k*log_2p_k$$     $$Ent(D)$$的值越小，D的纯度越高。

$$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$        例:$$D^1(色泽=青绿)，D^ 2（色泽=乌黑）$$

信息增益越大，意味着使用属性α 来进行划分所获得的"纯度提升"越大

2、信息增益率：

$$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$        $$IV(a)=\sum\limits_{v=1}^{V}\frac{|D^v|}{|D|}*log_2\frac{|D^v|}{|D|}$$

信息增益：对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响。

信息增益率：对可取值数目较少的属性有所偏好 

先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

3、基尼指数

基尼值：$$Gini(D)=1-\sum\limits_{k=1}^{N}p_k^ 2$$   $$Gini(D)$$越小，数据集D的纯度越高。

$$Gini\_index(D,a)=\sum\limits_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$$    基尼指数最小的属性为最优划分属性

##### 三、剪枝（pruning）处理

剪枝是决策树算法对付**"过拟合"**的手段

1、预剪枝 (pre pruning)：指在决策树生成过程中，对每个结点在划分前先进行估计，若**当前结点的划分不能带来决策树泛化性能提升**，则**停止划分**并将当前结点标记为叶结点。

2、后剪枝（post pruning)：是先从训练集生成一棵完整的决策树，然后**自底向上**地对**非叶结点**进行考察，若将该结点**对应的子树替换为叶结点**能带来**决策树泛化性能提升**，则将该子树替换为叶结点。

##### 四、连续与缺失值

1、连续值处理：采用二分法对连续值进行处理

2、缺失值处理：$$Gain(D,a) = p-Gain(D,a)$$      ρ 表辰无缺失值样本所占的比例。

##### 五、多变量决策树 ：

决策树所形成的分类边界有一个明显的特点:**轴平行(axis-parallel)** ，即它的分类边界由若干个与坐标轴平行的分段组成。

多变量决策树学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器 。

##### 决策树生成算法：

##### 决策树优化策略：

##### 决策树可视化：

1、安装

```python
pip install graphviz #安装python的graphviz插件
pip install pydotplus #安装python的pydotplus插件
```

2、可视化方式：

- 将模型输出dot文件，然后使用graphviz命令将dot文件转换为pdf格式的文件
- 直接使用pydotplus插件直接生成pdf文件进行保存
- 使用Image对象直接显示pydotplus生成的图片